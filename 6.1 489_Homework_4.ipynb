{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build the links for crawling\n",
    "home  = \"http://www.ipaidabribe.com/reports/all#gsc.tab=0\"\n",
    "left  = \"http://www.ipaidabribe.com/reports/all?page=\"\n",
    "right = \"#gsc.tab=0\"\n",
    "mid   = np.arange(10, 1000, 10)\n",
    "links = [home]\n",
    "for x in mid:\n",
    "    temp = str(left) + str(x) + str(right)\n",
    "    links.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n",
      "Processing page...\n"
     ]
    }
   ],
   "source": [
    "# Get all the pages for extraction\n",
    "pages = []\n",
    "for x in links:\n",
    "    print(\"Processing page...\")\n",
    "    html = requests.get(str(x)).text\n",
    "    soup = BeautifulSoup(html, 'html5lib')\n",
    "    pages.append(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the titles from the bribes taken.\n",
    "# This is only for a single website.\n",
    "title      = []\n",
    "date       = []\n",
    "bribe      = []\n",
    "department = []\n",
    "views      = []\n",
    "city       = []\n",
    "\n",
    "for soup in pages:\n",
    "    # Title of the offense.\n",
    "    title_raw = soup.find_all(\"h3\")\n",
    "    for x in title_raw:\n",
    "        temp = x.text\n",
    "        temp = temp.split()\n",
    "        temp = \" \".join(temp)\n",
    "        title.append(temp)\n",
    "    \n",
    "    # Date of the offense.\n",
    "    date_raw   = soup.find_all(\"span\", class_=\"date\") \n",
    "    for x in date_raw:\n",
    "        date.append(x.text)\n",
    "\n",
    "    # Amount of bribe.\n",
    "    bribe_raw = soup.find_all(\"li\", class_=\"paid-amount\")\n",
    "    for x in bribe_raw:\n",
    "        temp = x.text\n",
    "        temp = temp.split()\n",
    "        temp = \" \".join(temp)\n",
    "        temp = temp.split(\"INR\")[-1]\n",
    "        temp = temp.replace(\",\", \"\")\n",
    "        bribe.append(int(temp))\n",
    "    \n",
    "    # Name of department\n",
    "    department_raw = soup.find_all(\"li\", class_=\"transaction\")\n",
    "    for x in department_raw:\n",
    "        temp = x.text\n",
    "        temp = temp.split()\n",
    "        temp = \" \".join(temp)\n",
    "        department.append(temp)\n",
    "    \n",
    "    # Number of views\n",
    "    views_raw = soup.find_all(\"li\", class_=\"views\")\n",
    "    for x in views_raw:\n",
    "        temp = x.text\n",
    "        temp = temp.replace(\" views\", \"\")\n",
    "        views.append(int(temp))\n",
    "    \n",
    "    # City\n",
    "    city_raw = soup.find_all(\"a\", class_=\"location\")\n",
    "    for x in city_raw:\n",
    "        temp = x.text\n",
    "        temp = temp.split()\n",
    "        temp = \" \".join(temp)\n",
    "        temp = temp.replace(\" ,\", \",\")\n",
    "        city.append(temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
