{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Before beginning, I'll create a dummy variable for analysis.\n",
    "\n",
    "Currently, there's no dummy variable.\n",
    "I'd really enjoy being able to ctrl+enter on all Jupyter notebooks\n",
    "paired with a solid set of example data to show off the methods.\n",
    "\"\"\"\n",
    "from numpy     import *\n",
    "from functools import partial\n",
    "import math\n",
    "\n",
    "# The following commands are from Linear Algebra\n",
    "def vector_subtract (v, w):\n",
    "    return [v_i - w_i for v_i, w_i in zip(v,w)]\n",
    "\n",
    "def dot(v, w):\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v,w))\n",
    "\n",
    "def scalar_multiply(c, v):\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def sum_of_squares(v):\n",
    "    return dot(v, v)\n",
    "\n",
    "def squared_distance(v, w):\n",
    "    return sum_of_squares(vector_subtract(v, w))\n",
    "\n",
    "def distance(v, w):\n",
    "    return math.sqrt(squared_distance(v, w))\n",
    "# This is taken from a separate learning library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Suppose we have some function f that takes as input a vector\n",
    "of real numbers and outputs a single real number. One simple\n",
    "such function is the sum of squares:\n",
    "\"\"\"\n",
    "def sum_of_squares(v):\n",
    "    return sum(v_i ** 2 for v_i in v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If f is a function of one variable, its derivative at point x measures\n",
    "how f(x) changes when we make a very small change to x. It is defined as\n",
    "the limit of the difference quotients.\n",
    "\n",
    "The idea is that estimating a derivative is much more computationally \n",
    "mindful and gets us to approximately similar outcomes given the data\n",
    "is well structured.\n",
    "\"\"\"\n",
    "def difference_quotient(f, x, h):\n",
    "    return (f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEICAYAAABcVE8dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x893ba90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "What if you couldn't (or didn't want to) find the gradient? Although\n",
    "we can't take limit in Python, we can estimate derivatives by \n",
    "evaluating the difference quotient for a very small epsilon.\n",
    "\n",
    "Here is an example:\n",
    "\"\"\"\n",
    "def square(x):\n",
    "    return x*x\n",
    "\n",
    "def derivative(x):\n",
    "    return 2*x\n",
    "\n",
    "derivative_estimate = partial(difference_quotient, square, h=0.00001)\n",
    "\n",
    "# plot to show that they're basically the same\n",
    "import matplotlib.pyplot as plt\n",
    "x = range(-10, 10)\n",
    "plt.title(\"Actual Derivatives vs. Estimates\")\n",
    "plt.plot(x, map(derivative, x), 'rx', label='Actual')\n",
    "plt.plot(x, map(derivative_estimate, x), 'b+', label='Estimate')\n",
    "plt.legend(loc=9)\n",
    "plt.show()\n",
    "\n",
    "# As you can see, the fit lines are actually nearly identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "When f is a function of many variables, it has multiple partial derivatives,\n",
    "each indicating how f changes when we make small changes in just one of the\n",
    "input variables.\n",
    "\n",
    "We calculate its i'th partal derivative by treating it as a function of\n",
    "just its i'th variable, holding all other variables fixed:\n",
    "\"\"\"\n",
    "def partial_difference_quotient(f, v, i, h):\n",
    "    w = [v_j + (h if j == i else 0)\n",
    "         for j, v_j in enumerate(v)]\n",
    "    return (f(w) - f(v)) / h\n",
    "\n",
    "def estimate_gradient(f, v, h=0.00001):\n",
    "    return [partial_difference_quotient(f, v, i, h)\n",
    "            for i, _ in enumerate(v)]\n",
    "\n",
    "# Note, this method has a drawback: O(2n) rather than O(n) or O(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.5372437838531188e-06, -1.7686218919265594e-06, -2.9477031532109354e-06]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sometimes, we want to find the local minima of a gradient in a 3D space.\n",
    "In order to do that, we can compute the sum of squares of the vector,\n",
    "and then take a negative gradient step until we converge on the tolerance\n",
    "area that we've specified.\n",
    "\"\"\"\n",
    "def step(v, direction, step_size):\n",
    "    return [v_i + step_size * direction_i\n",
    "            for v_i, direction_i in zip(v, direction)]\n",
    "\n",
    "def sum_of_squares_gradient(v):\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "# Pick a random starting point\n",
    "v = [random.randint(-10, 10) for i in range(3)]\n",
    "\n",
    "tolerance = 0.0000001\n",
    "\n",
    "while True:\n",
    "    gradient = sum_of_squares_gradient(v)\n",
    "    next_v   = step(v, gradient, -0.01)\n",
    "    if distance(next_v, v) < tolerance:\n",
    "        break\n",
    "    v = next_v\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-27-dabed13c8ccb>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-27-dabed13c8ccb>\"\u001b[1;36m, line \u001b[1;32m15\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In order to put all of this together, we can define the function\n",
    "minimize_batch, which is the empirical way of calculating gradient\n",
    "descent. Unfortunately, batch processing is quite slow. Luckily, \n",
    "in the next dialogue box, we'll be going over Stochastic Gradient Descent\n",
    "which is a randomized, more computationally efficient form of descension.\n",
    "\n",
    "target_fn   is the function we'd like to minimize     \n",
    "            = can be square, whatever function\n",
    "\n",
    "gradient_fn is the calculated gradient of the function \n",
    "            = sum_of_squares_gradient(v)\n",
    "            \n",
    "theta_0     is the (user) given starting value         \n",
    "            = [random.randint(-10, 10) for i in range(3)]\n",
    "\n",
    "The description of the function is:\n",
    "\"Use gradient descent to find theta that minimizes target function\"\n",
    "\"\"\"\n",
    "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "    \n",
    "    theta     = theta_0\n",
    "    target_fn = safe(target_fn)\n",
    "    value     = target_fn(theta)\n",
    "    \n",
    "    while True:\n",
    "        gradient    = gradient_fn(theta)\n",
    "        next_thetas = [step(theta, gradient, -step_size)\n",
    "                       for step_size in step_sizes]\n",
    "        \n",
    "        # Choose the one that minimizes the error function\n",
    "        next_theta = min(next_thetas, key=target_fn)\n",
    "        next_value = target_fn(next_theta)\n",
    "        \n",
    "        # Stop if we're converging\n",
    "        if abs(value - next_value) < tolerance:\n",
    "            return theta\n",
    "        else:\n",
    "            theta, value = next_theta, next_value\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sometimes, we need to maximize this batch descent, or rather, we need\n",
    "to ascend the gradient rather than descend. We can do this by simply\n",
    "negating the functions we've written\n",
    "\"\"\"\n",
    "def negate(f):\n",
    "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
    "\n",
    "def negate_all(f):\n",
    "    return lambda *args, *kwargs: [-y for y in f(*args, **kwargs)\n",
    "                                   \n",
    "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    return minimize_batch(negate(target_fn),\n",
    "                          negate_all(gradient_fn),\n",
    "                          theta_0,\n",
    "                          tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Stochastic Gradient Descent computes the gradient (and takes a step) \n",
    "utilizing only one variable at a time. This process iterates and descends\n",
    "in random order on the entire data set until the total change in the dataset\n",
    "is less than the given tolerance.\n",
    "\n",
    "First, we'll set up our data to iterate in random order:\n",
    "\"\"\"\n",
    "def in_random_order(data):\n",
    "    indexes = [i for i, _ in enumerate(data)]\n",
    "    random.shuffle(indexes)\n",
    "    for i in indexes:\n",
    "        yield data[i]\n",
    "        \n",
    "# The following functions were created in the Linear Algebra section\n",
    "\n",
    "# End of Linear Algebra functions\n",
    "        \n",
    "# Then, we'll want to take a gradient step for every data point.\n",
    "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "    data                           = zip(x, y)\n",
    "    theta                          = theta_0\n",
    "    alpha                          = alpha_0\n",
    "    min_theta, min_value           = None, float(\"inf\")\n",
    "    iterations_with_no_improvement = 0\n",
    "    \n",
    "    while iterations_with_no_improvement < 100:\n",
    "        value = sum( target_fn(x_i, y_i, theta) for x_i, y_i in data)\n",
    "        \n",
    "        if value < min_value:\n",
    "            # We've found a new minimum; remember it,\n",
    "            # and continue on to the subsequent step size.\n",
    "            min_theta, min_value = theta, value\n",
    "            iterations_with_no_improvement = 0\n",
    "            alpha = alpha_0\n",
    "        else:\n",
    "            # No new minimum found\n",
    "            iterations_with_no_improvement += 1\n",
    "            alpha *= 0.9\n",
    "        \n",
    "        # Gradient step\n",
    "        for x_i, y_i in in_random_order(data):\n",
    "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
    "            theta      = vector_subtract(theta, \n",
    "                                         scalar_multiply(alpha, gradient_i))\n",
    "        return min_theta\n",
    "    \n",
    "# As an added bonus, here's the ascension on the gradient:\n",
    "def maximize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "    return minimize_stochastic(negate(target_fn),\n",
    "                               negate_all(gradient_fn),\n",
    "                               x, y, theta_0, alpha_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
