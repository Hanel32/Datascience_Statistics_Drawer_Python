{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Myers Briggs Social Media Personality Classifier\n",
    "----\n",
    "\n",
    "Aspects to note:\n",
    "----\n",
    "- The dataset is predominately introverts.\n",
    "- Feeling vs. Thinking seems to be the most evenly split feature.\n",
    "- Judging vs. Perceiving is a decent 60/40 split, where it seems perception is higher.\n",
    "- There are very few Sensing people; more Intuitive.\n",
    "\n",
    "Goals:\n",
    "----\n",
    "- Remove links from the dataset.\n",
    "- Analyze the sentiment associated with each text\n",
    "- Analyze the similarity between texts of the same personality type\n",
    "- Mine underlying features in the text by applying known feature extraction\n",
    "- Sequence model by looking at the bi and trigram structure\n",
    "- (Potential) Score the academic level of speaking of the speaker\n",
    "\n",
    "Things to try:\n",
    "----\n",
    "- Train a classifier with data split up as \"E\" vs. \"I\" vs. \"S\" etc. in unaries.\n",
    "- Train a classifier with data split up as \"ES\" vs.  \"SF\" vs. \"TP\" etc. in binaries\n",
    "- Train a classifier with data split up as full type, e.g. \"ENTJ\"\n",
    "- Utilize those three classifiers, trained on mined features, to get an aggregate vote of classification.\n",
    "\n",
    "Metrics to explain:\n",
    "----\n",
    "- Mechanics behind feature engineering\n",
    "- Data split; i.e., assumptions and bias added by example counts.\n",
    "- Principle component analysis of features.\n",
    "- Clustering analysis of personality types.\n",
    "- Support Vector Machine model on the features\n",
    "- F1 recall of our model\n",
    "- Precision/Recall tradeoff\n",
    "- ROC curve\n",
    "- AUC metric to compare different classifiers accuracy on the data\n",
    "- Confusion matrix to show where data misclassified\n",
    "- Talk about future features to be mined from those insights\n",
    "\n",
    "Future:\n",
    "----\n",
    "- Apply this text analysis to Russian twitter bots.\n",
    "- Apply this text analysis to Political talk forums.\n",
    "- Apply this text to a mutual interest group forum.\n",
    "- Apply this text to a social media group setting.\n",
    "- Analyze the variance and see if any insight can be gained\n",
    "\n",
    "First, I'll import the necessary libraries.\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, import the dataset.\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>'I'm finding the lack of me in these posts ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>'You're fired.|||That's another silly misconce...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                              posts\n",
       "0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n",
       "1  ENTP  'I'm finding the lack of me in these posts ver...\n",
       "2  INTP  'Good one  _____   https://www.youtube.com/wat...\n",
       "3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n",
       "4  ENTJ  'You're fired.|||That's another silly misconce..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data      = pd.read_csv(\"datasets/mbti_1.csv\")  # Reading in the data\n",
    "safe_data = data                                # Extra copy incase something bad happens.\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the regular expression, and split the text correctly.\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8675 individuals represented in the dataset.\n",
      "There are 50 tweets from each individual represented.\n"
     ]
    }
   ],
   "source": [
    "data      = pd.read_csv(\"datasets/mbti_1.csv\")  # Reading in the data\n",
    "clean_tweets = []\n",
    "\n",
    "# Split into the actual tweets\n",
    "for person in data['posts']:\n",
    "    examples       = person.split(\"|||\")\n",
    "    clean_tweets.append(examples)\n",
    "\n",
    "# Put those clean tweets back into the dataset\n",
    "data['posts'] = clean_tweets\n",
    "data.head()\n",
    "\n",
    "# Print some dimensions\n",
    "print(\"There are \" + str(len(clean_tweets)) + \" individuals represented in the dataset.\")\n",
    "print(\"There are \" + str(len(clean_tweets[0])) + \" tweets from each individual represented.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data explanation at this point:\n",
    "----\n",
    "Now, the data has been split into 50 tweets for each of 8675 individuals.\n",
    "\n",
    "Let's check out how the data is distributed over the types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1999 individuals (23.0432276657%) classified as Extroverted.\n",
      "There are 6676 individuals (76.9567723343%) classified as Introverted.\n",
      "\n",
      "There are 1197 individuals (13.7982708934%) classified as Sensing.\n",
      "There are 7478 individuals (86.2017291066%) classified as Intuitive.\n",
      "\n",
      "There are 4694 individuals (54.1095100865%) classified as Feeling.\n",
      "There are 3981 individuals (45.8904899135%) classified as Thinking.\n",
      "\n",
      "There are 3434 individuals (39.5850144092%) classified as Judging.\n",
      "There are 5241 individuals (60.4149855908%) classified as Perceiving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Get a count for each of the data types.\n",
    "type_count = list(Counter(data['type']).items())\n",
    "\n",
    "# E/I count\n",
    "E_raw   = [x for x in type_count if 'E' in x[0]]\n",
    "E_count = sum([x[1] for x in E_raw])\n",
    "print(\"There are \" + str(E_count) + \" individuals (\" +\n",
    "      str(float(int(E_count)/8675.)*100.) + \"%) classified as Extroverted.\")\n",
    "print(\"There are \" + str(int(8675 - E_count)) + \" individuals (\" +\n",
    "      str(float(int(8675 - E_count)/8675.)*100.) + \"%) classified as Introverted.\")\n",
    "print(\"\")\n",
    "\n",
    "# N/S count\n",
    "N_raw   = [x for x in type_count if 'S' in x[0]]\n",
    "N_count = sum([x[1] for x in N_raw])\n",
    "print(\"There are \" + str(N_count) + \" individuals (\" + \n",
    "      str(float(int(N_count)/8675.)*100.) + \"%) classified as Sensing.\")\n",
    "print(\"There are \" + str(int(8675 - N_count)) + \" individuals (\" +\n",
    "      str(float(int(8675 - N_count)/8675.)*100.) + \"%) classified as Intuitive.\")\n",
    "print(\"\")\n",
    "\n",
    "# F/T count\n",
    "F_raw   = [x for x in type_count if 'F' in x[0]]\n",
    "F_count = sum([x[1] for x in F_raw])\n",
    "print(\"There are \" + str(F_count) + \" individuals (\" +\n",
    "      str(float(int(F_count)/8675.)*100.) + \"%) classified as Feeling.\")\n",
    "print(\"There are \" + str(int(8675 - F_count)) + \" individuals (\" +\n",
    "      str(float(int(8675 - F_count)/8675.)*100.) + \"%) classified as Thinking.\")\n",
    "print(\"\")\n",
    "\n",
    "# J/P count\n",
    "J_raw   = [x for x in type_count if 'J' in x[0]]\n",
    "J_count = sum([x[1] for x in J_raw])\n",
    "print(\"There are \" + str(J_count) + \" individuals (\" +\n",
    "      str(float(int(J_count)/8675.)*100.) + \"%) classified as Judging.\")\n",
    "print(\"There are \" + str(int(8675 - J_count)) + \" individuals (\" +\n",
    "      str(float(int(8675 - J_count)/8675.)*100.) + \"%) classified as Perceiving.\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then let's see how it's spread out by actual classification:\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENFJ 190\n",
      "ESFP 48\n",
      "INFJ 1470\n",
      "ESTJ 39\n",
      "ISTJ 205\n",
      "ENTJ 231\n",
      "ISFP 271\n",
      "INTJ 1091\n",
      "ISTP 337\n",
      "ENTP 685\n",
      "ISFJ 166\n",
      "INTP 1304\n",
      "ESFJ 42\n",
      "ESTP 89\n",
      "ENFP 675\n",
      "INFP 1832\n"
     ]
    }
   ],
   "source": [
    "# Cleanly print the class, count tuples.\n",
    "for a, b in type_count:\n",
    "    print(str(a) + \" \" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll remove the link data\n",
    "----\n",
    "Luckily, as you can see, every individual represented in the dataset had related media linked. This allows for much deeper future analysis on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of individuals with links in their posts: 8675\n",
      "Number of individuals with texts in their posts: 8675\n",
      "\n",
      "In total, 25231 posts were removed as link data.\n",
      "There were a total of 433750 posts in the data.\n",
      "0.0581694524496 percent of the data contains a link.\n",
      "\n",
      "Individual 0 first 10 texts:\n",
      "\n",
      "['What has been the most life-changing experience in your life?', 'May the PerC Experience immerse you.', \"Hello ENFJ7. Sorry to hear of your distress. It's only natural for a relationship to not be perfection all the time in every moment of existence. Try to figure the hard times as times of growth, as...\", 'Welcome and stuff.', \"Prozac, wellbrutin, at least thirty minutes of moving your legs (and I don't mean moving them while sitting in your same desk chair), weed in moderation (maybe try edibles as a healthier alternative...\", \"Basically come up with three items you've determined that each type (or whichever types you want to do) would more than likely use, given each types' cognitive functions and whatnot, when left by...\", 'All things in moderation.  Sims is indeed a video game, and a good one at that. Note: a good one at that is somewhat subjective in that I am not completely promoting the death of any given Sim...', 'Dear ENFP:  What were your favorite video games growing up and what are your now, current favorite video games? :cool:', 'It appears to be too late. :sad:', \"There's someone out there for everyone.\"]\n",
      "\n",
      "Individual 0 first 10 links:\n",
      "\n",
      "[\"'http://www.youtube.com/watch?v=qsXHcwe3krw\", 'http://41.media.tumblr.com/tumblr_lfouy03PMA1qa1rooo1_500.jpg', 'enfp and intj moments  https://www.youtube.com/watch?v=iz7lE1g4XM4  sportscenter not top ten plays  https://www.youtube.com/watch?v=uCdfze1etec  pranks', 'http://www.youtube.com/watch?v=vXZeYwwRDw8   http://www.youtube.com/watch?v=u8ejam5DP3E  On repeat for most of today.', 'The last thing my INFJ friend posted on his facebook before committing suicide the next day. Rest in peace~   http://vimeo.com/22842206', '84389  84390  http://wallpaperpassion.com/upload/23700/friendship-boy-and-girl-wallpaper.jpg  http://assets.dornob.com/wp-content/uploads/2010/04/round-home-design.jpg ...', 'http://playeressence.com/wp-content/uploads/2013/08/RED-red-the-pokemon-master-32560474-450-338.jpg  Game. Set. Match.', 'https://www.youtube.com/watch?v=QyPqT8umzmY', 'http://www.youtube.com/watch?v=gDhy7rdfm14  I really dig the part from 1:46 to 2:50', 'http://www.youtube.com/watch?v=msqXffgh7b8']\n",
      "\n",
      "Individual 0's personality type: INFJ\n"
     ]
    }
   ],
   "source": [
    "# Take the dataset and remove the links, separate into links and texts.\n",
    "links = [] # Hold all link data\n",
    "texts = [] # Hold all text data\n",
    "\n",
    "# Start parsing the dataset by individual\n",
    "for individual in data['posts']:\n",
    "    txt = []\n",
    "    img = []\n",
    "    \n",
    "    # For each individual, split into text vs. link\n",
    "    for post in individual:\n",
    "        if \"http\" not in post:\n",
    "            img.append(post)\n",
    "        else:\n",
    "            txt.append(post)\n",
    "    \n",
    "    # Append the data to the data pool.\n",
    "    links.append(txt)\n",
    "    texts.append(img)\n",
    "\n",
    "# Confirm that the data is still represented correctly.\n",
    "print(\"Number of individuals with links in their posts: \" + str(len(links)))\n",
    "print(\"Number of individuals with texts in their posts: \" + str(len(texts)))\n",
    "print(\"\")\n",
    "\n",
    "# Find out how many posts were left over\n",
    "removed = 0\n",
    "for individual in links:\n",
    "    removed += len(individual)\n",
    "print(\"In total, \" + str(removed) + \" posts were removed as link data.\")\n",
    "print(\"There were a total of \" + str(8675 * 50) + \" posts in the data.\")\n",
    "print(str(float(removed) / float(8675 * 50)) + \" percent of the data contains a link.\\n\")\n",
    "\n",
    "print(\"Individual 0 first 10 texts:\\n\")\n",
    "print(str(texts[0][:10]) + \"\\n\")\n",
    "print(\"Individual 0 first 10 links:\\n\")\n",
    "print(str(links[0][:10]) + \"\\n\")\n",
    "print(\"Individual 0's personality type: \" + str(data['type'][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, the link data makes up so little of the dataset that we've not lost much information\n",
    "----\n",
    "Also, the dimensions are the same as our original dataset, so we can place the solely text data into the dataset.\n",
    "\n",
    "Let's return some structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Text</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>[What has been the most life-changing experien...</td>\n",
       "      <td>['http://www.youtube.com/watch?v=qsXHcwe3krw, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>['I'm finding the lack of me in these posts ve...</td>\n",
       "      <td>[http://img188.imageshack.us/img188/6422/6020d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>INTP</td>\n",
       "      <td>[Of course, to which I say I know; that's my b...</td>\n",
       "      <td>['Good one  _____   https://www.youtube.com/wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>['Dear INTP,   I enjoyed our conversation the ...</td>\n",
       "      <td>[Sx as hell...   https://www.youtube.com/watch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>['You're fired., That's another silly misconce...</td>\n",
       "      <td>[Sometimes I just really like impoverished rap...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type                                               Text  \\\n",
       "0  INFJ  [What has been the most life-changing experien...   \n",
       "1  ENTP  ['I'm finding the lack of me in these posts ve...   \n",
       "2  INTP  [Of course, to which I say I know; that's my b...   \n",
       "3  INTJ  ['Dear INTP,   I enjoyed our conversation the ...   \n",
       "4  ENTJ  ['You're fired., That's another silly misconce...   \n",
       "\n",
       "                                                Link  \n",
       "0  ['http://www.youtube.com/watch?v=qsXHcwe3krw, ...  \n",
       "1  [http://img188.imageshack.us/img188/6422/6020d...  \n",
       "2  ['Good one  _____   https://www.youtube.com/wa...  \n",
       "3  [Sx as hell...   https://www.youtube.com/watch...  \n",
       "4  [Sometimes I just really like impoverished rap...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the wanted variables\n",
    "labels = data['type']\n",
    "\n",
    "split_db = pd.DataFrame({'Type'      : labels,\n",
    "                         'Text'      : texts,\n",
    "                         'Link'      : links},\n",
    "                       columns=['Type','Text','Link'])\n",
    "\n",
    "split_db.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start mining some features within the text\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource u'tokenizers/punkt/english.pickle' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\Carson/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\Carson\\\\Anaconda2\\\\nltk_data'\n    - 'C:\\\\Users\\\\Carson\\\\Anaconda2\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Carson\\\\AppData\\\\Roaming\\\\nltk_data'\n    - u''\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-f86a6d3f2589>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindividual\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mpost\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindividual\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mpost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Carson\\Anaconda2\\lib\\site-packages\\nltk\\tokenize\\__init__.pyc\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \"\"\"\n\u001b[1;32m---> 96\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Carson\\Anaconda2\\lib\\site-packages\\nltk\\data.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Carson\\Anaconda2\\lib\\site-packages\\nltk\\data.pyc\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Carson\\Anaconda2\\lib\\site-packages\\nltk\\data.pyc\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    651\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 653\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource u'tokenizers/punkt/english.pickle' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\Carson/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'C:\\\\Users\\\\Carson\\\\Anaconda2\\\\nltk_data'\n    - 'C:\\\\Users\\\\Carson\\\\Anaconda2\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Carson\\\\AppData\\\\Roaming\\\\nltk_data'\n    - u''\n**********************************************************************"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import tokenize\n",
    "\n",
    "for individual in texts:\n",
    "    for post in individual:\n",
    "        post = tokenize.sent_tokenize(post)\n",
    "        \n",
    "print(texts[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
